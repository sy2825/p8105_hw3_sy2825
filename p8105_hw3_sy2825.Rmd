---
title: "P8105_hw3_sy2825"
author: "Shuo Yan (sy2825)"
output: github_document
date: "2018-10-12"
---


```{r setup, include = FALSE}
library(tidyverse)
```

# Problem 1

First, let's import and clean the BRFSS data.

```{r BRFSS_data_import_and_clean}
library(p8105.datasets)

data(brfss_smart2010)

brfss_clean = janitor::clean_names(brfss_smart2010) %>%
  rename(state = locationabbr, state_and_county = locationdesc, lower_confidence_limit = confidence_limit_low, 
         higher_confidence_limit = confidence_limit_high) %>%
  filter(topic == "Overall Health") %>%
  mutate(
    response = factor(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))
  )
  
  
 
brfss_clean

```

Now we can use our dataset to answer the following questions.

*In 2002, which states were observed at 7 locations?

```{r states_been_observed_at_7_locations}
brfss_clean %>%
  filter(year == "2002") %>%
  group_by(state) %>%
  summarize(n_locations = n_distinct(state_and_county)) %>%
  filter(n_locations == 7)
```

From the table we can see that states CT, FL, and NC were observed at 7 locations.

*Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r spaghetti_plot_n_locations}
brfss_clean %>%
  group_by(year, state) %>%
  summarize(n_locations = n_distinct(state_and_county)) %>%
  ggplot(aes(x = year, y = n_locations)) +
  geom_line(aes(color = state)) +
  labs(
    title = "Number of locations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of locations",
    caption = "Data from BRFSS data in p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "States", 
    discrete = TRUE
  )

```
From the spaghetti plot above we can see that state FL has big differences of the number of observed locations from 2002 to 2010.

*Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r mean_and_sd_of_proportion_of_excellent}
brfss_clean %>%
  filter(year == "2002" | year == "2006" | year == "2010", state == "NY", response == "Excellent") %>%
  group_by(Year = year) %>%
  summarize(Mean = mean(data_value)/100, Standard_deviation = sd(data_value)/100) %>%
  knitr::kable(digits = 2)
  
```

From the table we can see that the mean and standard deviation of the proportion of "Excellent" responses across locations in NY state have few differences between year 2002, 2006, and 2010.

*For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r five_panel_plot}
brfss_average_proportion = brfss_clean %>%
  group_by(Year = year, State = state, Response = response) %>%
  summarize(Proportion = mean(data_value)/100)

brfss_average_proportion

brfss_average_proportion %>%
ggplot(aes(x = Year, y = Proportion, color = State)) +
  geom_line() +
  labs(
    title = "Distribution of average proportion over time",
    x = "Year",
    y = "Proportion",
    caption = "Data from BRFSS data in p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "States", 
    discrete = TRUE
  ) +
  facet_grid(. ~ Response) +
  theme(legend.position = "bottom")

```

From the plots wo can see that "very good" response has highest proportion.

# Problem 2

First, let's import and clean the instacart data.

```{r instacart}

data(instacart)

instacart_clean = janitor::clean_names(instacart)

```
Then let's explore this dataset.


```{r instacart_explore}

instacart_clean

information_explore = matrix(c(
  nrow(distinct(instacart_clean, product_name)), nrow(distinct(instacart_clean, product_id)),
  
  nrow(distinct(instacart_clean, aisle)), nrow(distinct(instacart_clean, aisle_id)),
  
  nrow(distinct(instacart_clean, department)), nrow(distinct(instacart_clean, department_id))), ncol = 3
)

colnames(information_explore) = c("Product", "Aisle", "Department")
rownames(information_explore) = c("Count of different names", "Count of different id")

as.table(information_explore)
```


We can see that this dataset includes the order information about 39123 different food product and also the identity information of each product such as product name, product id, aisle, and department. There are 134 different aisles and 21 departments in total and each aisle and department has its unique id number.

*How many aisles are there, and which aisles are the most items ordered from?

```{r aisle}
sort(
  table(instacart_clean$aisle), decreasing = TRUE
  )[1]

```

As we concluded before, there are 134 different aisles intotal. "Fresh vegetables"" is the aisle which the most items ordered from.

*Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r aisle_plot}
aisle_data = instacart_clean %>%
  group_by(aisle) %>%
  summarize(n_items = n())

aisle_part_1 = head(aisle_data, 30)

aisle_part_1 %>%
ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 1)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  ) 

aisle_part_2 = slice(aisle_data, 31:60)

aisle_part_2 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 2)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

aisle_part_3 = slice(aisle_data, 61:90)

aisle_part_3 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 3)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

aisle_part_4 = slice(aisle_data, 91:120)

aisle_part_4 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 4)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

aisle_part_5 = slice(aisle_data, 121:134)

aisle_part_5 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 5)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

```

*Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r aisle_table}
 instacart_clean %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
group_by(Aisle = aisle, Most_popular_product = product_name) %>% 
  summarize(n_items = n()) %>%
  mutate(Product_rank = min_rank(desc(n_items))) %>% 
  filter(Product_rank == 1) %>%
  select(Aisle, Most_popular_product) %>%
  knitr::kable()

```

*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r mean_hour_table}
instacart_clean %>%
  mutate(order_dow = recode(order_dow, 
    "0" = "Monday", "1" = "Tuesday", "2" = "Wednesday", "3" = "Thursday", "4" = "Friday",
    "5" = "Saturday", "6" = "Sunday")) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(Product_name = product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)

```


