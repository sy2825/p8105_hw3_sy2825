---
title: "P8105_hw3_sy2825"
author: "Shuo Yan (sy2825)"
output: github_document
date: "2018-10-12"
---


```{r setup, include = FALSE}
library(tidyverse)
```

# Problem 1

First, let's import and clean the BRFSS data.

```{r BRFSS_data_import_and_clean}
library(p8105.datasets)

data(brfss_smart2010)

brfss_clean = janitor::clean_names(brfss_smart2010) %>%
  rename(state = locationabbr, state_and_county = locationdesc, lower_confidence_limit = confidence_limit_low, 
         higher_confidence_limit = confidence_limit_high) %>%
  filter(topic == "Overall Health") %>%
  mutate(
    response = factor(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))
  )
  
  
 
brfss_clean

```

Now we can use our dataset to answer the following questions.

*In 2002, which states were observed at 7 locations?

```{r states_been_observed_at_7_locations}
brfss_clean %>%
  filter(year == "2002") %>%
  group_by(state) %>%
  summarize(n_locations = n_distinct(state_and_county)) %>%
  filter(n_locations == 7)
```

From the table we can see that states CT, FL, and NC were observed at 7 locations.

*Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r spaghetti_plot_n_locations}
brfss_clean %>%
  group_by(year, state) %>%
  summarize(n_locations = n_distinct(state_and_county)) %>%
  ggplot(aes(x = year, y = n_locations)) +
  geom_line(aes(color = state)) +
  labs(
    title = "Number of locations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of locations",
    caption = "Data from BRFSS data in p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "States", 
    discrete = TRUE
  )

```
From the spaghetti plot above we can see that state FL has big differences of the number of observed locations from 2002 to 2010.

*Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r mean_and_sd_of_proportion_of_excellent}
brfss_clean %>%
  filter(year == "2002" | year == "2006" | year == "2010", state == "NY", response == "Excellent") %>%
  group_by(Year = year) %>%
  summarize(Mean = mean(data_value)/100, Standard_deviation = sd(data_value)/100) %>%
  knitr::kable(digits = 2)
  
```

From the table we can see that the mean and standard deviation of the proportion of "Excellent" responses across locations in NY state have few differences between year 2002, 2006, and 2010.

*For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r five_panel_plot}
brfss_average_proportion = brfss_clean %>%
  group_by(Year = year, State = state, Response = response) %>%
  summarize(Proportion = mean(data_value)/100)

brfss_average_proportion

brfss_average_proportion %>%
ggplot(aes(x = Year, y = Proportion, color = State)) +
  geom_line() +
  labs(
    title = "Distribution of average proportion over time",
    x = "Year",
    y = "Proportion",
    caption = "Data from BRFSS data in p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "States", 
    discrete = TRUE
  ) +
  facet_grid(. ~ Response) +
  theme(legend.position = "bottom")

```

From the plots wo can see that "very good" response has highest proportion.

# Problem 2

First, let's import and clean the instacart data.

```{r instacart}

data(instacart)

instacart_clean = janitor::clean_names(instacart)

```
Then let's explore this dataset.


```{r instacart_explore}

dim(instacart_clean)

instacart_clean

information_explore = matrix(c(
  nrow(distinct(instacart_clean, product_name)), nrow(distinct(instacart_clean, product_id)),
  
  nrow(distinct(instacart_clean, aisle)), nrow(distinct(instacart_clean, aisle_id)),
  
  nrow(distinct(instacart_clean, department)), nrow(distinct(instacart_clean, department_id))), ncol = 3
)

colnames(information_explore) = c("Product", "Aisle", "Department")
rownames(information_explore) = c("Count of different names", "Count of different id")

as.table(information_explore)
```


We can see that this 1384617 * 15 dataset includes the order information about 39123 different food product and also the identity information of each product such as product name, product id, aisle, and department. There are 134 different aisles and 21 departments in total and each aisle and department has its unique id number.

*How many aisles are there, and which aisles are the most items ordered from?

```{r aisle}
sort(
  table(instacart_clean$aisle), decreasing = TRUE
  )[1]

```

As we concluded before, there are 134 different aisles intotal. "Fresh vegetables"" is the aisle which the most items ordered from.

*Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r aisle_plot}
aisle_data = instacart_clean %>%
  group_by(aisle) %>%
  summarize(n_items = n())

aisle_part_1 = head(aisle_data, 30)

aisle_part_1 %>%
ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 1)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  ) 

aisle_part_2 = slice(aisle_data, 31:60)

aisle_part_2 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 2)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

aisle_part_3 = slice(aisle_data, 61:90)

aisle_part_3 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 3)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

aisle_part_4 = slice(aisle_data, 91:120)

aisle_part_4 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 4)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

aisle_part_5 = slice(aisle_data, 121:134)

aisle_part_5 %>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Items ordered in each aisle (Part 5)",
    x = "Aisle",
    y = "Number of items ordered in each aisle",
    caption = "Data from instacart data in p8105.datasets package"
  )

```

*Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r aisle_table}
 instacart_clean %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
group_by(Aisle = aisle, Most_popular_product = product_name) %>% 
  summarize(n_items = n()) %>%
  mutate(Product_rank = min_rank(desc(n_items))) %>% 
  filter(Product_rank == 1) %>%
  select(Aisle, Most_popular_product) %>%
  knitr::kable()

```

*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r mean_hour_table}
instacart_clean %>%
  mutate(order_dow = recode(order_dow, 
    "0" = "Monday", "1" = "Tuesday", "2" = "Wednesday", "3" = "Thursday", "4" = "Friday",
    "5" = "Saturday", "6" = "Sunday")) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(Product_name = product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)

```

# Problem 3

First, let's import and have a look at the NY NOAA data.

```{r NY_NOAA}
data(ny_noaa)

 ny_noaa
 
dim(ny_noaa)

ny_noaa_distinct_value = matrix(
  c(
    nrow(distinct(ny_noaa, id)), nrow(distinct(ny_noaa, date))
    ), ncol = 2)

colnames(ny_noaa_distinct_value) = c("id", "date")
rownames(ny_noaa_distinct_value) = c("Count of distinct value")

as.table(ny_noaa_distinct_value)

ny_noaa_na = matrix(c(
  
  sum(is.na(ny_noaa$id)), sum(is.na(ny_noaa$date)),
  
  sum(is.na(ny_noaa$prcp)), sum(is.na(ny_noaa$snow)),
  
  sum(is.na(ny_noaa$snwd)), sum(is.na(ny_noaa$tmax)),
  
  sum(is.na(ny_noaa$tmin))), ncol = 7
)

colnames(ny_noaa_na) = c("id", "date", "prcp", "snow", "snwd", "tmax", "tmin")
rownames(ny_noaa_na) = c("Count of NA")

as.table(ny_noaa_na)

```

We can see that this is a 2595176 * 7 dataset containg the variables of:

*id: Weather station ID

*date: Date of observation

*prcp: Precipitation (tenths of mm)

*snow: Snowfall (mm)

*snwd: Snow depth (mm)

*tmax: Maximum temperature (tenths of degrees C)

*tmin: Minimum temperature (tenths of degrees C)

This dataset collect the information about 747 different id in 10957 different days. We can see there are lots of NA value in the dataset so I have made a table to count the NA value in each variable. From the table we know that nearly half of the examined days don't have tmax value and it's the same situation for tmin. Fortunately, we don't have any missing values for id and date.

*Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r ny_noaa_clean}

ny_noaa_clean = janitor::clean_names(ny_noaa) %>%
  mutate(
    year = lubridate::year(date),
    month = month.name[lubridate::month(date)],
    day = lubridate::day(date),
    prcp = as.numeric(prcp)/10,
    tmax = as.numeric(tmax)/10,
    tmin = as.numeric(tmin)/10) %>%

    rename(precipitation_mm = prcp) %>%
    rename(snow_depth_mm = snwd) %>%
    rename(snowfall_mm = snow) %>%
    rename(tmax_c = tmax) %>%
    rename(tmin_c = tmin)

ny_noaa_clean
```

 Now we have a cleaned data, then we will find the most commonly observed values for snowfall.
 
```{r snowfall}
ny_noaa_clean %>%
  group_by(snowfall_mm) %>%
  summarize(n = n()) %>%
  filter(n == max(n))
```

We can see that the most commonly observed value for snowfall in NY is 0 mm, this is beacuse most of days in NY do not have snowfall.

*Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r average_max_temperature}
ny_noaa_clean %>%
  filter(month == "January" | month == "July") %>%
  filter(!is.na(tmax_c)) %>%
  group_by(year, month, id) %>%
  summarize(avg_tmax = mean(tmax_c)) %>%
  ggplot(aes(x = factor(year), y = avg_tmax, color = month)) +
  geom_boxplot() +
  facet_grid(. ~ month) +
   theme(axis.text.x = element_text(angle = 90)) +
  labs(
    title = "Average max Temperature in January and in July",
    x = "Year",
    y = "Average max temperature in degrees C",
    caption = "Data from ny_noaa data in p8105.datasets package"
  )
```

From the plot we can see that July is more likely to have higher temperature than January. However, outliers exist in most of year for both January and July, this is because of the suddenly increasing or decreasing of the temperature.

*Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.


