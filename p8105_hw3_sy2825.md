P8105\_hw3\_sy2825
================
Shuo Yan (sy2825)
2018-10-12

Problem 1
=========

First, let's import and clean the BRFSS data.

``` r
library(p8105.datasets)

data(brfss_smart2010)

brfss_clean = janitor::clean_names(brfss_smart2010) %>%
  rename(state = locationabbr, state_and_county = locationdesc, lower_confidence_limit = confidence_limit_low, 
         higher_confidence_limit = confidence_limit_high) %>%
  filter(topic == "Overall Health") %>%
  mutate(
    response = factor(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))
  )
  
  
 
brfss_clean
```

    ## # A tibble: 10,625 x 23
    ##     year state state_and_county class topic question response sample_size
    ##    <int> <chr> <chr>            <chr> <chr> <chr>    <fct>          <int>
    ##  1  2010 AL    AL - Jefferson ~ Heal~ Over~ How is ~ Excelle~          94
    ##  2  2010 AL    AL - Jefferson ~ Heal~ Over~ How is ~ Very go~         148
    ##  3  2010 AL    AL - Jefferson ~ Heal~ Over~ How is ~ Good             208
    ##  4  2010 AL    AL - Jefferson ~ Heal~ Over~ How is ~ Fair             107
    ##  5  2010 AL    AL - Jefferson ~ Heal~ Over~ How is ~ Poor              45
    ##  6  2010 AL    AL - Mobile Cou~ Heal~ Over~ How is ~ Excelle~          91
    ##  7  2010 AL    AL - Mobile Cou~ Heal~ Over~ How is ~ Very go~         177
    ##  8  2010 AL    AL - Mobile Cou~ Heal~ Over~ How is ~ Good             224
    ##  9  2010 AL    AL - Mobile Cou~ Heal~ Over~ How is ~ Fair             120
    ## 10  2010 AL    AL - Mobile Cou~ Heal~ Over~ How is ~ Poor              66
    ## # ... with 10,615 more rows, and 15 more variables: data_value <dbl>,
    ## #   lower_confidence_limit <dbl>, higher_confidence_limit <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

Now we can use our dataset to answer the following questions.

\*In 2002, which states were observed at 7 locations?

``` r
brfss_clean %>%
  filter(year == "2002") %>%
  group_by(state) %>%
  summarize(n_locations = n_distinct(state_and_county)) %>%
  filter(n_locations == 7)
```

    ## # A tibble: 3 x 2
    ##   state n_locations
    ##   <chr>       <int>
    ## 1 CT              7
    ## 2 FL              7
    ## 3 NC              7

From the table we can see that states CT, FL, and NC were observed at 7 locations.

\*Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

``` r
brfss_clean %>%
  group_by(year, state) %>%
  summarize(n_locations = n_distinct(state_and_county)) %>%
  ggplot(aes(x = year, y = n_locations)) +
  geom_line(aes(color = state)) +
  labs(
    title = "Number of locations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of locations",
    caption = "Data from BRFSS data in p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "States", 
    discrete = TRUE
  )
```

![](p8105_hw3_sy2825_files/figure-markdown_github/spaghetti_plot_n_locations-1.png) From the spaghetti plot above we can see that state FL has big differences of the number of observed locations from 2002 to 2010.

\*Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
brfss_clean %>%
  filter(year == "2002" | year == "2006" | year == "2010", state == "NY", response == "Excellent") %>%
  group_by(Year = year) %>%
  summarize(Mean = mean(data_value)/100, Standard_deviation = sd(data_value)/100) %>%
  knitr::kable(digits = 2)
```

|  Year|  Mean|  Standard\_deviation|
|-----:|-----:|--------------------:|
|  2002|  0.24|                 0.04|
|  2006|  0.23|                 0.04|
|  2010|  0.23|                 0.04|

From the table we can see that the mean and standard deviation of the proportion of "Excellent" responses across locations in NY state have few differences between year 2002, 2006, and 2010.

\*For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

``` r
brfss_average_proportion = brfss_clean %>%
  group_by(Year = year, State = state, Response = response) %>%
  summarize(Proportion = mean(data_value)/100)

brfss_average_proportion
```

    ## # A tibble: 2,215 x 4
    ## # Groups:   Year, State [?]
    ##     Year State Response  Proportion
    ##    <int> <chr> <fct>          <dbl>
    ##  1  2002 AK    Excellent      0.279
    ##  2  2002 AK    Very good      0.337
    ##  3  2002 AK    Good           0.238
    ##  4  2002 AK    Fair           0.086
    ##  5  2002 AK    Poor           0.059
    ##  6  2002 AL    Excellent      0.185
    ##  7  2002 AL    Very good      0.309
    ##  8  2002 AL    Good           0.327
    ##  9  2002 AL    Fair           0.121
    ## 10  2002 AL    Poor           0.059
    ## # ... with 2,205 more rows

``` r
brfss_average_proportion %>%
ggplot(aes(x = Year, y = Proportion, color = State)) +
  geom_line() +
  labs(
    title = "Distribution of average proportion over time",
    x = "Year",
    y = "Proportion",
    caption = "Data from BRFSS data in p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "States", 
    discrete = TRUE
  ) +
  facet_grid(. ~ Response) +
  theme(legend.position = "bottom")
```

    ## Warning: Removed 1 rows containing missing values (geom_path).

![](p8105_hw3_sy2825_files/figure-markdown_github/five_panel_plot-1.png)

From the plots wo can see that "very good" response has highest proportion.

Problem 2
=========

First, let's import and clean the instacart data.

``` r
data(instacart)

instacart_clean = janitor::clean_names(instacart)
```

Then let's explore this dataset.

``` r
instacart_clean
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord~ reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # ... with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

``` r
information_explore = matrix(c(
  nrow(distinct(instacart_clean, product_name)), nrow(distinct(instacart_clean, product_id)),
  
  nrow(distinct(instacart_clean, aisle)), nrow(distinct(instacart_clean, aisle_id)),
  
  nrow(distinct(instacart_clean, department)), nrow(distinct(instacart_clean, department_id))), ncol = 3
)

colnames(information_explore) = c("Product", "Aisle", "Department")
rownames(information_explore) = c("Count of different names", "Count of different id")

as.table(information_explore)
```

    ##                          Product Aisle Department
    ## Count of different names   39123   134         21
    ## Count of different id      39123   134         21

We can see that this dataset includes the order information about 39123 different food product and also the identity information of each product such as product name, product id, aisle, and department. There are 134 different aisles and 21 departments in total and each aisle and department has its unique id number.
